# Pr√°ctica 4 y 4b

**Autores:**  
- Laura Herrera Negr√≠n  
- Ayman Asbai Ghoudan

**Universidad:** Universidad de Las Palmas de Gran Canaria  
**Asignatura:** Visi√≥n por Computador  

---
## Contenidos
- [Librer√≠as utilizadas](#librerias)
- [Pr√°ctica 4 - Detecci√≥n de veh√≠culos y matr√≠culas](#pr√°ctica4) 
    - [Preparaci√≥n del dataset para YOLO](#dataset) 
    - [Entrenamiento YOLO](#entrenamiento)
- [Pr√°ctica 4b - ](#tarea2)
---

<a name= "librerias"></a>
## Librer√≠as utilizadas

--- 

<a name="pr√°ctica4"></a>
## Pr√°ctica 4 - Detecci√≥n de veh√≠culos y matr√≠culas
El objetivo de esta pr√°ctica es desarrollar un prototipo para detectar y seguir veh√≠culos y personas, as√≠ como la localizaci√≥n y reconocimiento de las matr√≠culas de dichos veh√≠culos a partir de un v√≠deo. Para ello, se han empleado modelos de detecci√≥n de objetos YOLO (You Only Look Once).

<a name="entorno"></a>
### Prearaci√≥n del entorno
Para evitar conflictos entre librer√≠as y garantizar la compatibilidad con el m√≥dulo de **OCR** utilizado posteriormente, se cre√≥ un nuevo entorno de **Conda** con **Python 3.9.5**:
```bash
conda create --name VC_P4 python=3.9.5
conda activate VC_P4
conda install pytorch torchvision torchaudio pytorch-cuda=11.8 -c pytorch -c nvidia
pip install ultralytics
pip install lap
```
La tercera instrucci√≥n instala PyTorch junto con sus librer√≠as asociadas (torchvision y torchaudio) y habilita el soporte de CUDA 11.8 para aprovechar la aceleraci√≥n por GPU.

El paquete Ultralytics permite acceder a las versiones m√°s recientes de YOLO (YOLOv11 y YOLOv12), facilitando tanto el uso de modelos preentrenados como el entrenamiento de modelos personalizados.
  
<a name= "dataset"></a>
### Preparaci√≥n del dataset para YOLO
Para la detecci√≥n de matr√≠culas, se decidi√≥ entrenar un modelo YOLO personalizado, ya que los modelos preentrenados no incluyen esta clase de objeto por defecto.

El proceso seguido fue el siguiente:
#### 1. Obtenci√≥n y preparaci√≥n del dataset
Se recopil√≥ un conjunto de im√°genes que contuvieran veh√≠culos con matr√≠culas visibles.  
Este dataset fue creado de forma colaborativa entre los miembros del equipo de la asignatura, garantizando la variedad de condiciones (√°ngulos, iluminaci√≥n, tipos de veh√≠culos, etc.).  
En total, se recopilaron 150 im√°genes, que se guardaron en la carpeta [_todo_](https://github.com/lauraheerrera/VC/tree/P4/Practica4/todo), para posteriormente etiquetarlas y, una vez etiquetadas organizarlas siguiendo la estructura de YOLO.

#### 2. Anotaci√≥n de im√°genes
Para anotar las matr√≠culas dentro de las im√°genes se utiliz√≥ la herramienta **LabelMe**, que permite dibujar regiones rectangulares alrededor del objeto de inter√©s (la matr√≠cula).  

Para el uso de esta herramienta, se cre√≥ otro entorno:
```bash
conda create --name=labelme python=3.9
conda activate labelme
pip install labelme
```
Una vez instalado, se tecle√≥ _labelme_ desde _AnacondaPrompt_ y se abri√≥ una interfaz intuitiva para anotar las zonas alrededor de las matr√≠culas presentes.

Cada imagen anotada genera un archivo `.json` con la informaci√≥n de las regiones seleccionadas.  

#### 3. Estructura de directorios
Las im√°genes recolectadas se organizaron siguiendo la estructura esperada por **YOLO** para el entrenamiento, validaci√≥n y prueba del modelo.  
Cada subconjunto contiene sus respectivas carpetas de im√°genes (`images/`) y etiquetas (`labels/`).

<pre>
üìÇ <a href="https://github.com/lauraheerrera/VC/tree/P4/Practica4/TGC_RBNW">TGC_RBNW/</a>
‚îú‚îÄ‚îÄ üìÇ <a href="https://github.com/lauraheerrera/VC/tree/P4/Practica4/TGC_RBNW/train">train/</a>
‚îÇ   ‚îú‚îÄ‚îÄ <a href="https://github.com/lauraheerrera/VC/tree/P4/Practica4/TGC_RBNW/train/images">images/</a>
‚îÇ   ‚îî‚îÄ‚îÄ <a href="https://github.com/lauraheerrera/VC/tree/P4/Practica4/TGC_RBNW/train/labels">labels/</a>
‚îú‚îÄ‚îÄ üìÇ <a href="https://github.com/lauraheerrera/VC/tree/P4/Practica4/TGC_RBNW/val">val/</a>
‚îÇ   ‚îú‚îÄ‚îÄ <a href="https://github.com/lauraheerrera/VC/tree/P4/Practica4/TGC_RBNW/val/images">images/</a>
‚îÇ   ‚îî‚îÄ‚îÄ <a href="https://github.com/lauraheerrera/VC/tree/P4/Practica4/TGC_RBNW/val/labels">labels/</a>
‚îî‚îÄ‚îÄ üìÇ <a href="https://github.com/lauraheerrera/VC/tree/P4/Practica4/TGC_RBNW/test">test/</a>
    ‚îú‚îÄ‚îÄ <a href="https://github.com/lauraheerrera/VC/tree/P4/Practica4/TGC_RBNW/test/images">images/</a>
    ‚îî‚îÄ‚îÄ <a href="https://github.com/lauraheerrera/VC/tree/P4/Practica4/TGC_RBNW/test/labels">labels/</a>
</pre>

Para crear esta estructura, se desarroll√≥, con ayuda de la IA, un [**script en Python**](https://github.com/lauraheerrera/VC/blob/P4/Practica4/repartir.py) que tom√≥ todas las im√°genes y etiquetas almacenadas inicialmente en la carpeta  
[_todo_](https://github.com/lauraheerrera/VC/tree/P4/Practica4/todo) y las dividi√≥ en tres subconjuntos de forma autom√°tica:
- **80%** del total del dataset se destin√≥ a **entrenamiento y validaci√≥n**.  
- **20%** restante se reserv√≥ para **pruebas (test)**.  
- Del **80% inicial**, se dividi√≥ de nuevo en:
  - **80%** para **entrenamiento (train)**
  - **20%** para **validaci√≥n (val)**

De esta forma, se garantiza una distribuci√≥n equilibrada y representativa del dataset, cumpliendo con las pr√°cticas recomendadas para el entrenamiento de modelos de detecci√≥n de objetos.

#### 4. De `json` a formato YOLO
Una vez creada la estructura de carpetas del dataset, es importante recordar  que las anotaciones generadas con **LabelMe** se guardan inicialmente en formato `.json`. Para que el modelo **YOLO** pueda utilizarlas, es necesario convertirlas al formato de etiquetas propio del framework.

Para ello, se desarroll√≥ un [**script en Python**](https://github.com/lauraheerrera/VC/blob/P4/Practica4/script.py) que recorre todas las etiquetas en formato `.json` y las convierte en archivos `.txt` con la estructura est√°ndar de YOLO:
`<class_id> <x_center> <y_center> <width> <height>`

Cada l√≠nea del archivo `.txt` corresponde a un objeto detectado dentro de la imagen y contiene la siguiente informaci√≥n:
- **class_id** ‚Üí identificador num√©rico de la clase del objeto (por ejemplo, `0` para matr√≠culas).  
- **x_center** ‚Üí coordenada **x** del centro del contenedor delimitador.  
- **y_center** ‚Üí coordenada **y** del centro del contenedor delimitador.  
- **width** ‚Üí ancho del contenedor delimitador.  
- **height** ‚Üí alto del contenedor delimitador.  

Las coordenadas del centro (`x_center`, `y_center`) y las dimensiones (`width`, `height`) se encuentran **normalizadas**, es decir, divididas por el ancho y alto total de la imagen, para que sus valores est√©n comprendidos entre 0 y 1.

De esta forma, las etiquetas resultantes son totalmente compatibles con los modelos **YOLO**, permitiendo entrenar el detector de matr√≠culas de manera eficiente.

#### 5. Archivo de configuraci√≥n del dataset

Se cre√≥ un archivo [`data.yaml`](https://github.com/lauraheerrera/VC/blob/P4/Practica4/data.yaml), que define las rutas del conjunto de datos utilizadas durante el entrenamiento, validaci√≥n y prueba del modelo.  
Adem√°s, este archivo especifica el n√∫mero de clases y sus nombres, informaci√≥n necesaria para que **YOLO** interprete correctamente el dataset.

El contenido del archivo tiene la siguiente estructura:
```yaml
# TGCRBNW paths

train: C:/Users/laura/OneDrive/Desktop/VC/Practica4/TGC_RBNW/train/
val: C:/Users/laura/OneDrive/Desktop/VC/Practica4/TGC_RBNW/val/
test: C:/Users/laura/OneDrive/Desktop/VC/Practica4/TGC_RBNW/test/

# number of classes
nc: 1

# class names
names: [ 'license_plate' ]
```
---

<a name= "entrenamiento"></a>
### Proceso para el entrenamiento YOLO
A continuaci√≥n, se entrenar√° el modelo YOLO:
#### 1. Activar el entorno para entrenamiento si no se ha hecho previamente
`conda activate VC_P4`

#### 2. Comprobar que la GPU est√° disponible
```bash
python -c "import torch; print('Torch version:', torch.__version__); print('CUDA available:', torch.cuda.is_available()); print('GPU:', torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'None')"
```

#### 3. Ejecutar entrenamiento YOLO
Desde la carpeta donde est√° `data.yaml` y las im√°genes:
```
cd "C:\Users\laura\OneDrive\Desktop\VC\Practica4"
```
1. **Train1 (tama√±o de imagen 416, 40 √©pocas)**
```bash
yolo detect train model=yolo11n.pt data=data.yaml imgsz=416 batch=4 device=0 epochs=40
```
2. **Train2 (tama√±o de imagen 512, 40 √©pocas)**
```bash
yolo detect train model=yolo11n.pt data=data.yaml imgsz=512 batch=4 device=0 epochs=40
```
3. Train3 (tama√±o de imagen 416, 100 √©pocas)
```bash
yolo detect train model=yolo11n.pt data=data.yaml imgsz=416 batch=4 device=0 epochs=100
```
4. Train4 (tama√±o de imagen 512, 100 √©pocas)
```bash
yolo detect train model=yolo11n.pt data=data.yaml imgsz=512 batch=4 device=0 epochs=100
```
Par√°metros:
- `model` ‚Üí modelo base/preentrenado (`yolo11n.pt`)
- `data` ‚Üí archivo YAML con rutas y clases
- `imgsz` ‚Üí tama√±o de entrada de las im√°genes
- `batch` ‚Üí tama√±o de batch por iteraci√≥n
- `device=0` ‚Üí GPU utilizada
- `epochs` ‚Üí n√∫mero de √©pocas de entrenamiento
  
El objetivo de los distintos entrenamientos es analizar c√≥mo el tama√±o de entrada y el n√∫mero de √©pocas afectan la precisi√≥n y la capacidad de detecci√≥n del modelo sobre matr√≠culas, para elegir la configuraci√≥n √≥ptima.

<a name="resultados"></a>
### Resultados del entrenamiento
Tras ejecutar los distintos entrenamientos, YOLO genera autom√°ticamente los resultados en la carpeta:
Dentro de esta carpeta, se crean subcarpetas por cada ejecuci√≥n, por ejemplo `train1`, `train2`, etc. Cada subcarpeta contiene los siguientes elementos:
- **`weights/`** ‚Üí Modelos entrenados:
  - **`best.pt`** ‚Üí Modelo que obtuvo la mejor precisi√≥n durante el entrenamiento.  
  - **`last.pt`** ‚Üí Modelo final despu√©s de completar todas las √©pocas, aunque no sea el m√°s preciso.
- **`results.png`** ‚Üí Gr√°fica que muestra la evoluci√≥n de las m√©tricas de entrenamiento: precisi√≥n, recall y loss.

