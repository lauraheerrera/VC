# Pr√°ctica 4 y 4b

**Autores:**  
- Laura Herrera Negr√≠n  
- Ayman Asbai Ghoudan

**Universidad:** Universidad de Las Palmas de Gran Canaria  
**Asignatura:** Visi√≥n por Computador  

---
## Contenidos
- [Librer√≠as utilizadas](#librerias)
- [Pr√°ctica 4 - Detecci√≥n de veh√≠culos y matr√≠culas](#pr√°ctica4) 
    - [Preparaci√≥n del dataset para YOLO](#dataset) 
    - [Entrenamiento YOLO](#entrenamiento)
    - [Resultados del entrenamiento](#resultados)
    - [Instrucciones para ejecutar el script](#script) 
- [Pr√°ctica 4b - ](#tarea2)
---

<a name="librerias"></a>
## Librer√≠as utilizadas
[![PyTorch](https://img.shields.io/badge/PyTorch-%23EE4C2C?style=for-the-badge&logo=pytorch&logoColor=white)](https://pytorch.org/)  
- Framework principal para entrenamiento de modelos YOLO.  
- Soporte de GPU mediante CUDA para acelerar el entrenamiento.  
- Incluye m√≥dulos como `torchvision` y `torchaudio` para manipulaci√≥n de datos multimodales.  

[![CUDA](https://img.shields.io/badge/CUDA-%230edc0f?style=for-the-badge&logo=nvidia&logoColor=white)](https://developer.nvidia.com/cuda-zone)  
- Librer√≠a de aceleraci√≥n por GPU utilizada por PyTorch.  

[![Ultralytics YOLO](https://img.shields.io/badge/Ultralytics%20YOLO-%23FF6F61?style=for-the-badge&logo=ultralytics&logoColor=white&labelColor=%23FF6F61)](https://github.com/ultralytics/ultralytics)
- Implementaci√≥n moderna de YOLO (YOLOv11).  
- Facilita entrenamiento, validaci√≥n y detecci√≥n de objetos con modelos preentrenados y personalizados.  

[![LabelMe](https://img.shields.io/badge/LabelMe-%23F6A623?style=for-the-badge&logo=labelme&logoColor=white)](https://github.com/wkentaro/labelme)  
- Herramienta gr√°fica para anotaci√≥n de im√°genes.  
- Generar archivos `.json` con las coordenadas de objetos (matr√≠culas).  

[![lap](https://img.shields.io/badge/lap-%23007ACC?style=for-the-badge)](https://pypi.org/project/lap/)  
- Librer√≠a para resolver problemas de asignaci√≥n lineal, √∫til en seguimiento de objetos.  

--- 

<a name="pr√°ctica4"></a>
## Pr√°ctica 4 - Detecci√≥n de veh√≠culos y matr√≠culas
El objetivo de esta pr√°ctica es desarrollar un prototipo para detectar y seguir veh√≠culos y personas, as√≠ como la localizaci√≥n y reconocimiento de las matr√≠culas de dichos veh√≠culos a partir de un v√≠deo. Para ello, se han empleado modelos de detecci√≥n de objetos YOLO (You Only Look Once).

<a name="entorno"></a>
### üñ•Ô∏è Prearaci√≥n del entorno
Para evitar conflictos entre librer√≠as y garantizar la compatibilidad con el m√≥dulo de **OCR** utilizado posteriormente, se cre√≥ un nuevo entorno de **Conda** con **Python 3.9.5**:
```bash
conda create --name VC_P4 python=3.9.5
conda activate VC_P4
conda install pytorch torchvision torchaudio pytorch-cuda=11.8 -c pytorch -c nvidia
pip install ultralytics
pip install lap
```
La tercera instrucci√≥n instala PyTorch junto con sus librer√≠as asociadas (torchvision y torchaudio) y habilita el soporte de CUDA 11.8 para aprovechar la aceleraci√≥n por GPU.

El paquete Ultralytics permite acceder a las versiones m√°s recientes de YOLO (YOLOv11 y YOLOv12), facilitando tanto el uso de modelos preentrenados como el entrenamiento de modelos personalizados.
  
<a name= "dataset"></a>
### üñºÔ∏è Preparaci√≥n del dataset para YOLO
Para la detecci√≥n de matr√≠culas, se decidi√≥ entrenar un modelo YOLO personalizado, ya que los modelos preentrenados no incluyen esta clase de objeto por defecto.

El proceso seguido fue el siguiente:
#### 1. Obtenci√≥n y preparaci√≥n del dataset
Se recopil√≥ un conjunto de im√°genes que contuvieran veh√≠culos con matr√≠culas visibles.  
Este dataset fue creado de forma colaborativa entre los miembros del equipo de la asignatura, garantizando la variedad de condiciones (√°ngulos, iluminaci√≥n, tipos de veh√≠culos, etc.).  
En total, se recopilaron 150 im√°genes, que se guardaron en la carpeta [_todo_](https://github.com/lauraheerrera/VC/tree/P4/Practica4/todo), para posteriormente etiquetarlas y, una vez etiquetadas organizarlas siguiendo la estructura de YOLO.

#### 2. Anotaci√≥n de im√°genes
Para anotar las matr√≠culas dentro de las im√°genes se utiliz√≥ la herramienta **LabelMe**, que permite dibujar regiones rectangulares alrededor del objeto de inter√©s (la matr√≠cula).  

Para el uso de esta herramienta, se cre√≥ otro entorno:
```bash
conda create --name=labelme python=3.9
conda activate labelme
pip install labelme
```
Una vez instalado, se tecle√≥ _labelme_ desde _AnacondaPrompt_ y se abri√≥ una interfaz intuitiva para anotar las zonas alrededor de las matr√≠culas presentes.

Cada imagen anotada genera un archivo `.json` con la informaci√≥n de las regiones seleccionadas.  

#### 3. Estructura de directorios
Las im√°genes recolectadas se organizaron siguiendo la estructura esperada por **YOLO** para el entrenamiento, validaci√≥n y prueba del modelo.  
Cada subconjunto contiene sus respectivas carpetas de im√°genes (`images/`) y etiquetas (`labels/`).

<pre>
üìÇ <a href="https://github.com/lauraheerrera/VC/tree/P4/Practica4/TGC_RBNW">TGC_RBNW/</a>
‚îú‚îÄ‚îÄ üìÇ <a href="https://github.com/lauraheerrera/VC/tree/P4/Practica4/TGC_RBNW/train">train/</a>
‚îÇ   ‚îú‚îÄ‚îÄ <a href="https://github.com/lauraheerrera/VC/tree/P4/Practica4/TGC_RBNW/train/images">images/</a>
‚îÇ   ‚îî‚îÄ‚îÄ <a href="https://github.com/lauraheerrera/VC/tree/P4/Practica4/TGC_RBNW/train/labels">labels/</a>
‚îú‚îÄ‚îÄ üìÇ <a href="https://github.com/lauraheerrera/VC/tree/P4/Practica4/TGC_RBNW/val">val/</a>
‚îÇ   ‚îú‚îÄ‚îÄ <a href="https://github.com/lauraheerrera/VC/tree/P4/Practica4/TGC_RBNW/val/images">images/</a>
‚îÇ   ‚îî‚îÄ‚îÄ <a href="https://github.com/lauraheerrera/VC/tree/P4/Practica4/TGC_RBNW/val/labels">labels/</a>
‚îî‚îÄ‚îÄ üìÇ <a href="https://github.com/lauraheerrera/VC/tree/P4/Practica4/TGC_RBNW/test">test/</a>
    ‚îú‚îÄ‚îÄ <a href="https://github.com/lauraheerrera/VC/tree/P4/Practica4/TGC_RBNW/test/images">images/</a>
    ‚îî‚îÄ‚îÄ <a href="https://github.com/lauraheerrera/VC/tree/P4/Practica4/TGC_RBNW/test/labels">labels/</a>
</pre>

Para crear esta estructura, se desarroll√≥, con ayuda de la IA, un [**script en Python**](https://github.com/lauraheerrera/VC/blob/P4/Practica4/repartir.py) que tom√≥ todas las im√°genes y etiquetas almacenadas inicialmente en la carpeta [_todo_](https://github.com/lauraheerrera/VC/tree/P4/Practica4/todo) y las dividi√≥ en tres subconjuntos de forma autom√°tica:
- **80%** del total del dataset se destin√≥ a **entrenamiento y validaci√≥n**.  
- **20%** restante se reserv√≥ para **pruebas (test)**.  
- Del **80% inicial**, se dividi√≥ de nuevo en:
  - **80%** para **entrenamiento (train)**
  - **20%** para **validaci√≥n (val)**
> [!IMPORTANT]
> Para ejecutar el script, sigue las instrucciones que se indican en la secci√≥n [Instrucciones para ejecutar el script](#script) ].

De esta forma, se garantiza una distribuci√≥n equilibrada y representativa del dataset, cumpliendo con las pr√°cticas recomendadas para el entrenamiento de modelos de detecci√≥n de objetos.

#### 4. De `json` a formato YOLO
Una vez creada la estructura de carpetas del dataset, es importante recordar  que las anotaciones generadas con **LabelMe** se guardan inicialmente en formato `.json`. Para que el modelo **YOLO** pueda utilizarlas, es necesario convertirlas al formato de etiquetas propio del framework.

Para ello, se desarroll√≥ un [**script en Python**](https://github.com/lauraheerrera/VC/blob/P4/Practica4/scripts/json_to_txt.py) que recorre todas las etiquetas en formato `.json` y las convierte en archivos `.txt` con la estructura est√°ndar de YOLO:
`<class_id> <x_center> <y_center> <width> <height>`

> [!IMPORTANT]
> Para ejecutar el script, sigue las instrucciones que se indican en la secci√≥n [Instrucciones para ejecutar el script](#script) ].

Cada l√≠nea del archivo `.txt` corresponde a un objeto detectado dentro de la imagen y contiene la siguiente informaci√≥n:
- **class_id** ‚Üí identificador num√©rico de la clase del objeto (por ejemplo, `0` para matr√≠culas).  
- **x_center** ‚Üí coordenada **x** del centro del contenedor delimitador.  
- **y_center** ‚Üí coordenada **y** del centro del contenedor delimitador.  
- **width** ‚Üí ancho del contenedor delimitador.  
- **height** ‚Üí alto del contenedor delimitador.  

Las coordenadas del centro (`x_center`, `y_center`) y las dimensiones (`width`, `height`) se encuentran **normalizadas**, es decir, divididas por el ancho y alto total de la imagen, para que sus valores est√©n comprendidos entre 0 y 1.

De esta forma, las etiquetas resultantes son totalmente compatibles con los modelos **YOLO**, permitiendo entrenar el detector de matr√≠culas de manera eficiente.

#### 5. Archivo de configuraci√≥n del dataset

Se cre√≥ un archivo [`data.yaml`](https://github.com/lauraheerrera/VC/blob/P4/Practica4/scripts/data.yaml), que define las rutas del conjunto de datos utilizadas durante el entrenamiento, validaci√≥n y prueba del modelo.  
Adem√°s, este archivo especifica el n√∫mero de clases y sus nombres, informaci√≥n necesaria para que **YOLO** interprete correctamente el dataset.

El contenido del archivo tiene la siguiente estructura:
```yaml
# TGCRBNW paths

train: C:/Users/laura/OneDrive/Desktop/VC/Practica4/TGC_RBNW/train/
val: C:/Users/laura/OneDrive/Desktop/VC/Practica4/TGC_RBNW/val/
test: C:/Users/laura/OneDrive/Desktop/VC/Practica4/TGC_RBNW/test/

# number of classes
nc: 1

# class names
names: [ 'license_plate' ]
```
---

<a name= "entrenamiento"></a>
### üìà Proceso para el entrenamiento YOLO
A continuaci√≥n, se entrenar√° el modelo YOLO:
#### 1. Activar el entorno para entrenamiento si no se ha hecho previamente
`conda activate VC_P4`

#### 2. Comprobar que la GPU est√° disponible
```bash
python -c "import torch; print('Torch version:', torch.__version__); print('CUDA available:', torch.cuda.is_available()); print('GPU:', torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'None')"
```

#### 3. Ejecutar entrenamiento YOLO
Desde la carpeta donde est√° `data.yaml` y las im√°genes:
```
cd "C:\Users\laura\OneDrive\Desktop\VC\Practica4"
```
1. Train 1 (T1) ‚Äì Entrenamiento r√°pido de referencia
```bash
yolo detect train model=yolo11n.pt data=data.yaml imgsz=512 batch=4 device=0 epochs=40 lr0=0.01
```
2. Train 2 (T2) ‚Äì Entrenamiento largo con im√°genes peque√±as
```bash
yolo detect train model=yolo11n.pt data=data.yaml imgsz=416 batch=4 device=0 epochs=100 lr0=0.001
```
3.  Train 3 (T3) ‚Äì Entrenamiento largo con resoluci√≥n media
```bash
yolo detect train model=yolo11n.pt data=data.yaml imgsz=512 batch=4 device=0 epochs=100 lr0=0.001
```
4. Train 4 (T4) ‚Äì Entrenamiento con im√°genes grandes y pocas √©pocas
```bash
yolo detect train model=yolo11n.pt data=data.yaml imgsz=640 batch=4 device=0 epochs=50 lr0=0.001
```
5. Train 5 (T5) ‚Äì Entrenamiento con batch grande
```
yolo detect train model=yolo11n.pt data=data.yaml imgsz=512 batch=8 device=0 epochs=60 lr0=0.001
```
6. Train 6 (T6) ‚Äì Repetici√≥n para comparar consistencia
```
yolo detect train model=yolo11n.pt data=data.yaml imgsz=416 batch=4 device=0 epochs=100 lr0=0.001
```
7. Train 7 (T7) ‚Äì Entrenamiento de alta resoluci√≥n, pocas √©pocas
```
yolo detect train model=yolo11n.pt data=data.yaml imgsz=768 batch=2 device=0 epochs=25 lr0=0.001
```
8. Train 8 (T8) ‚Äì Entrenamiento balanceado entre resoluci√≥n y duraci√≥n
```
yolo detect train model=yolo11n.pt data=data.yaml imgsz=640 batch=4 device=0 epochs=80 lr0=0.001
```
9. Train 9 (T9) ‚Äì Entrenamiento con learning rate alto
```
yolo detect train model=yolo11n.pt data=data.yaml imgsz=512 batch=4 device=0 epochs=100 lr0=0.01
```

Par√°metros:
- `model` ‚Üí modelo base/preentrenado (`yolo11n.pt`)
- `data` ‚Üí archivo YAML con rutas y clases (`data.yaml`)
- `imgsz` ‚Üí tama√±o de entrada de las im√°genes (512, 640, etc.)
- `batch` ‚Üí tama√±o de batch por iteraci√≥n (4, 8, etc.)
- `device=0` ‚Üí GPU utilizada (0 para la primera GPU, cpu si no hay GPU)
- `epochs` ‚Üí n√∫mero de √©pocas de entrenamiento (40, 100, etc.)
- `lr0` ‚Üí learning rate inicial para el entrenamiento (0.001, 0.01, etc.)
  
Se realizaron 9 entrenamientos para evaluar distintas combinaciones de tama√±o de imagen, n√∫mero de √©pocas, batch y learning rate:
- T1: Entrenamiento r√°pido de referencia, pocas √©pocas y tama√±o medio.
- T2: Largo con im√°genes peque√±as, para evaluar convergencia con menor detalle.
- T3: Largo con resoluci√≥n media, comparando precisi√≥n con T2.
- T4: Im√°genes grandes y pocas √©pocas, para capturar detalles sin mucho tiempo de entrenamiento.
- T5: Batch grande, probando estabilidad y suavidad de la convergencia.
- T6: Repetici√≥n de un entrenamiento largo, para validar consistencia de resultados.
- T7: Alta resoluci√≥n y pocas √©pocas, ideal para matr√≠culas peque√±as o lejanas.
- T8: Balance entre resoluci√≥n y duraci√≥n, buscando un modelo s√≥lido.
- T9: Learning rate alto, para observar efecto en velocidad de convergencia y estabilidad.

Este conjunto permite comparar c√≥mo cada par√°metro afecta la precisi√≥n y eficiencia del modelo de detecci√≥n de matr√≠culas.

<a name="resultados"></a>
### üìä Resultados del entrenamiento
Tras ejecutar los distintos entrenamientos, YOLO genera autom√°ticamente los resultados en la carpeta:
Dentro de esta carpeta, se crean subcarpetas por cada ejecuci√≥n, por ejemplo `train1`, `train2`, etc. Cada subcarpeta contiene los siguientes elementos:
- **`weights/`** ‚Üí Modelos entrenados:
  - **`best.pt`** ‚Üí Modelo que obtuvo la mejor precisi√≥n durante el entrenamiento.  
  - **`last.pt`** ‚Üí Modelo final despu√©s de completar todas las √©pocas, aunque no sea el m√°s preciso.
- **`results.png`** ‚Üí Gr√°fica que muestra la evoluci√≥n de las m√©tricas de entrenamiento: precisi√≥n, recall y loss.
- **`results.csv`** -> Registro √©poca por √©poca de las m√©tricas durante el entrenamiento y la validaci√≥n. Cada fila corresponde a una √©opca

Para determinar qu√© entrenamiento se considera el mejor, se ha tenido en cuenta las principales m√©tricas, que reflejan la calidad de la detecci√≥n:

| M√©trica                | Qu√© indica                                             | Consideraci√≥n para evaluaci√≥n                                                 |
| ---------------------- | ------------------------------------------------------ | ------------------------------------------------------------------ |
| `metrics/precision(B)` | Qu√© porcentaje de las predicciones fueron correctas    | M√°s alto = mejor                                                   |
| `metrics/recall(B)`    | Qu√© porcentaje de los objetos reales fueron detectados | M√°s alto = mejor                                                   |
| `metrics/mAP50(B)`     | Precisi√≥n promedio considerando IoU ‚â• 0.5              | M√°s alto = mejor; ideal >0.7 para muchos casos                     |
| `metrics/mAP50-95(B)`  | Precisi√≥n promedio considerando IoU entre 0.5 y 0.95   | M√°s robusta que mAP50, porque penaliza predicciones menos precisas |

Otras m√©tricas importantes son las de p√©rdidas, que reflejan qu√© tan bien aprende el modelo:
| M√©trica                           | Qu√© indica                                                  | Consideraci√≥n para evaluaci√≥n    |
| --------------------------------- | ----------------------------------------------------------- | ---------------- |
| `train/box_loss` y `val/box_loss` | Error de localizaci√≥n (qu√© tan bien encaja el bounding box) | M√°s bajo = mejor |
| `train/cls_loss` y `val/cls_loss` | Error de clasificaci√≥n (qu√© tan bien clasifica el objeto)   | M√°s bajo = mejor |
| `train/dfl_loss` y `val/dfl_loss` | Loss de distribuci√≥n focal (refina boxes)                   | M√°s bajo = mejor |

Para evaluar la calidad de un modelo, se ha priorizado las m√©tricas `*_best` de mAP y p√©rdidas de validaci√≥n, ya que reflejan el mejor rendimiento alcanzado durante el entrenamiento.

Sabiendo esto, se ha desarrolado otro [script de Python](https://github.com/lauraheerrera/VC/blob/P4/Practica4/scripts/guardar_metricas_yolo.py) que recorre autom√°ticamente todas las carpetas de entrenamiento (`train`,  `train2`, etc.), extrae las m√©tricas de cada ejecuci√≥n y genera un resumen de los mejores resultados para cada entrenamiento. Adem√°s, ordena autom√°ticamente los entrenamientos seg√∫n el siguiente criterio de prioridad, de manera que el primero en la lista corresponde al modelo mejor considerado:
1. `mAP50(B)` m√°s alto ‚Üí La m√©trica principal para determinar precisi√≥n de detecci√≥n.
2. `mAP50-95(B)` alto ‚Üí Eval√∫a robustez frente a predicciones menos perfectas.
3. Loss de validaci√≥n bajos (`val/box_loss`, `val/cls_loss`, `val/dfl_loss`) ‚Üí Indican que el modelo aprendi√≥ bien sin sobreajustarse.
4. Precision y recall equilibrados ‚Üí Evita falsos positivos o falsos negativos excesivos, asegurando un modelo confiable.


> [!IMPORTANT]
> Para ejecutar el script, sigue las instrucciones que se indican en la secci√≥n [Instrucciones para ejecutar el script](#script) ].

> De esta manera, al abrir el [Excel generado por el script](https://github.com/lauraheerrera/VC/blob/P4/Practica4/resumen_entrenamientos_mejores.xlsx), los entrenamientos aparecen ordenados seg√∫n estas prioridades, facilitando la identificaci√≥n del mejor modelo sin necesidad de revisar manualmente cada m√©trica.

La siguiente tabla muestra c√≥mo se presentan los entrenamientos en el _Excel_:
| Entrenamiento | train/box_loss_best | train/cls_loss_best | train/dfl_loss_best | val/box_loss_best | val/cls_loss_best | val/dfl_loss_best | metrics/precision(B)_best | metrics/recall(B)_best | metrics/mAP50(B)_best | metrics/mAP50-95(B)_best | val_loss_sum | pr_sum |
|---------------|------------------|-------------------|-------------------|-----------------|-----------------|-----------------|---------------------------|------------------------|----------------------|-------------------------|-------------|--------|
| train8        | 1.09635          | 1.2899            | 1.05174           | 0.87794         | 0.79134         | 0.96896         | 0.99986                   | 1.0000                 | 0.9950               | 0.74894                 | 2.63824     | 1.99986|
| train         | 0.96920          | 1.15768           | 0.98533           | 0.96195         | 0.83262         | 0.96145         | 0.99636                   | 1.0000                 | 0.9950               | 0.69216                 | 2.75602     | 1.99636|
| train3        | 0.96814          | 0.84988           | 0.95254           | 1.02529         | 0.75701         | 1.01806         | 0.99467                   | 1.0000                 | 0.9950               | 0.68005                 | 2.80036     | 1.99467|
| train9        | 0.96814          | 0.84988           | 0.95254           | 1.02529         | 0.75701         | 1.01806         | 0.99467                   | 1.0000                 | 0.9950               | 0.68005                 | 2.80036     | 1.99467|
| train2        | 0.72048          | 0.55734           | 0.89550           | 0.94331         | 0.60031         | 0.96383         | 1.00000                   | 0.94927                | 0.99204              | 0.70802                 | 2.50745     | 1.94927|
| train6        | 0.72048          | 0.55734           | 0.89550           | 0.94331         | 0.60031         | 0.96383         | 1.00000                   | 0.94927                | 0.99204              | 0.70802                 | 2.50745     | 1.94927|
| train4        | 0.83696          | 1.04597           | 0.90473           | 0.85140         | 0.76191         | 0.94132         | 0.95481                   | 1.0000                 | 0.99192              | 0.76919                 | 2.55463     | 1.95481|
| train5        | 0.74647          | 0.73564           | 0.91225           | 0.91703         | 0.70769         | 0.97270         | 0.96803                   | 0.9600                 | 0.99071              | 0.74413                 | 2.59742     | 1.92803|
| train7        | 0.94845          | 2.28946           | 0.95819           | 0.91889         | 1.43075         | 1.05861         | 0.95651                   | 0.9600                 | 0.98724              | 0.72895                 | 3.40825     | 1.91651|

Como se observa, ** `train8` es el modelo recomendado para la detecci√≥n de matr√≠culas**, y los resultados obtenidos servir√°n como referencia para optimizar y ajustar futuras iteraciones del entrenamiento de YOLO.

<a name= "script"></a>
### Instrucciones para ejecutar un script  
> Abre tu terminal  
> Sit√∫ate en la carpeta donde se encuentra el script. En mi caso: `cd "C:\Users\Laura\Desktop\VC\Practica 4"`  
> Ejecuta el script: `python <nombre_script>`


