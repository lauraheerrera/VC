# Pr√°ctica 4 y 4b

**Autores:**  
- Laura Herrera Negr√≠n  
- Ayman Asbai Ghoudan

**Universidad:** Universidad de Las Palmas de Gran Canaria  
**Asignatura:** Visi√≥n por Computador  

---
## Contenidos
- [Librer√≠as utilizadas](#librerias)
- [Pr√°ctica 4 - Detecci√≥n de veh√≠culos y matr√≠culas](#pr√°ctica4) 
    - [Preparaci√≥n del dataset para YOLO](#dataset) 
    - [Entrenamiento YOLO](#entrenamiento)
    - [Resultados del entrenamiento](#resultados)
    - [Instrucciones para ejecutar el script](#script) 
- [Pr√°ctica 4b - Reconocimiento de caracteres](#practica4b)
    - [Modelos OCR seleccionados](#OCR)
    - [Preparaci√≥n del entorno](#OCR-entorno)
    - [Proceso de reconocimiento de clases y caracteres](#proceso)
    - [Resultados modelos OCR](#resultados-4b)
    - [Diferencia de los tiempos de inferencia](#tiempos)
 
---

<a name="librerias"></a>
## Librer√≠as utilizadas
[![PyTorch](https://img.shields.io/badge/PyTorch-%23EE4C2C?style=for-the-badge&logo=pytorch&logoColor=white)](https://pytorch.org/)  
- Framework principal para entrenamiento de modelos YOLO.  
- Soporte de GPU mediante CUDA para acelerar el entrenamiento.  
- Incluye m√≥dulos como `torchvision` y `torchaudio` para manipulaci√≥n de datos multimodales.  

[![CUDA](https://img.shields.io/badge/CUDA-%230edc0f?style=for-the-badge&logo=nvidia&logoColor=white)](https://developer.nvidia.com/cuda-zone)  
- Librer√≠a de aceleraci√≥n por GPU utilizada por PyTorch.  

[![Ultralytics YOLO](https://img.shields.io/badge/Ultralytics%20YOLO-%23FF6F61?style=for-the-badge&logo=ultralytics&logoColor=white&labelColor=%23FF6F61)](https://github.com/ultralytics/ultralytics)
- Implementaci√≥n moderna de YOLO (YOLOv11).  
- Facilita entrenamiento, validaci√≥n y detecci√≥n de objetos con modelos preentrenados y personalizados.  

[![LabelMe](https://img.shields.io/badge/LabelMe-%23F6A623?style=for-the-badge&logo=labelme&logoColor=white)](https://github.com/wkentaro/labelme)  
- Herramienta gr√°fica para anotaci√≥n de im√°genes.  
- Generar archivos `.json` con las coordenadas de objetos (matr√≠culas).  

[![lap](https://img.shields.io/badge/lap-%23007ACC?style=for-the-badge)](https://pypi.org/project/lap/)  
- Librer√≠a para resolver problemas de asignaci√≥n lineal, √∫til en seguimiento de objetos.  

[![OpenCV](https://img.shields.io/badge/OpenCV-%23127C71?style=for-the-badge&logo=opencv&logoColor=white)](https://opencv.org/)  
- Procesamiento de im√°genes y videos.  
- Lectura/escritura de videos, manipulaci√≥n de frames, recorte de ROI, anotaciones gr√°ficas.  

[![NumPy](https://img.shields.io/badge/NumPy-%23013243?style=for-the-badge&logo=numpy&logoColor=white)](https://numpy.org/)  
- Manipulaci√≥n eficiente de arrays y operaciones matem√°ticas.  
- Soporte de c√°lculos matriciales y transformaciones de im√°genes.  

[![Pandas](https://img.shields.io/badge/Pandas-%23150458?style=for-the-badge&logo=pandas&logoColor=white)](https://pandas.pydata.org/)  
- Almacenamiento y manejo de datos en formato tabular.  
- Exportaci√≥n de resultados a CSV para an√°lisis posterior.  

[![Pytesseract](https://img.shields.io/badge/Pytesseract-%23000000?style=for-the-badge&logo=python&logoColor=white)](https://pypi.org/project/pytesseract/)  
- Wrapper de Tesseract OCR para Python.  
- Permite reconocimiento de texto en im√°genes, especialmente matr√≠culas.  

[![EasyOCR](https://img.shields.io/badge/EasyOCR-%23FF4F00?style=for-the-badge&logo=python&logoColor=white)](https://www.jaided.ai/easyocr/)  
- OCR moderno basado en redes neuronales profundas.  
- Reconocimiento de caracteres en im√°genes con buena velocidad y estabilidad.  

[![Time](https://img.shields.io/badge/Time-%23000000?style=for-the-badge&logo=python&logoColor=white)](https://docs.python.org/3/library/time.html)  
- Medici√≥n de tiempos de inferencia y procesamiento frame a frame.
--- 

<a name="pr√°ctica4"></a>
## Pr√°ctica 4 - Detecci√≥n de veh√≠culos y matr√≠culas
El objetivo de esta pr√°ctica es desarrollar un prototipo para detectar y seguir veh√≠culos y personas, as√≠ como la localizaci√≥n y reconocimiento de las matr√≠culas de dichos veh√≠culos a partir de un v√≠deo. Para ello, se han empleado modelos de detecci√≥n de objetos YOLO (You Only Look Once).

<a name="entorno"></a>
### üñ•Ô∏è Preparaci√≥n del entorno
Para evitar conflictos entre librer√≠as y garantizar la compatibilidad con el m√≥dulo de **OCR** utilizado posteriormente, se cre√≥ un nuevo entorno de **Conda** con **Python 3.9.5**:
```bash
conda create --name VC_P4 python=3.9.5
conda activate VC_P4
conda install pytorch torchvision torchaudio pytorch-cuda=11.8 -c pytorch -c nvidia
pip install ultralytics
pip install lap
```
La tercera instrucci√≥n instala PyTorch junto con sus librer√≠as asociadas (torchvision y torchaudio) y habilita el soporte de CUDA 11.8 para aprovechar la aceleraci√≥n por GPU.

El paquete Ultralytics permite acceder a las versiones m√°s recientes de YOLO (YOLOv11 y YOLOv12), facilitando tanto el uso de modelos preentrenados como el entrenamiento de modelos personalizados.
  
<a name= "dataset"></a>
### üñºÔ∏è Preparaci√≥n del dataset para YOLO
Para la detecci√≥n de matr√≠culas, se decidi√≥ entrenar un modelo YOLO personalizado, ya que los modelos preentrenados no incluyen esta clase de objeto por defecto.

El proceso seguido fue el siguiente:
#### 1. Obtenci√≥n y preparaci√≥n del dataset
Se recopil√≥ un conjunto de im√°genes que contuvieran veh√≠culos con matr√≠culas visibles.  
Este dataset fue creado de forma colaborativa entre los miembros del equipo de la asignatura, garantizando la variedad de condiciones (√°ngulos, iluminaci√≥n, tipos de veh√≠culos, etc.).  
En total, se recopilaron 150 im√°genes, que se guardaron en la carpeta _todo_, para posteriormente etiquetarlas y, una vez etiquetadas organizarlas siguiendo la estructura de YOLO.

Por motivos de espacio y buenas pr√°cticas, la carpeta del dataset (TGC_RBNW/) se ha a√±adido al archivo .gitignore, por lo que no forma parte del repositorio.
No obstante, el dataset completo puede consultarse o descargarse desde el siguiente enlace: [Enlace dataset](https://drive.google.com/drive/folders/1vX6mWZiZlpHOmNURqQK7ZL78xI9JnJk9?usp=sharing)

#### 2. Anotaci√≥n de im√°genes
Para anotar las matr√≠culas dentro de las im√°genes se utiliz√≥ la herramienta **LabelMe**, que permite dibujar regiones rectangulares alrededor del objeto de inter√©s (la matr√≠cula).  

Para el uso de esta herramienta, se cre√≥ otro entorno:
```bash
conda create --name=labelme python=3.9
conda activate labelme
pip install labelme
```
Una vez instalado, se tecle√≥ _labelme_ desde _AnacondaPrompt_ y se abri√≥ una interfaz intuitiva para anotar las zonas alrededor de las matr√≠culas presentes.

Cada imagen anotada genera un archivo `.json` con la informaci√≥n de las regiones seleccionadas.  

#### 3. Estructura de directorios
Las im√°genes recolectadas se organizaron siguiendo la estructura esperada por **YOLO** para el entrenamiento, validaci√≥n y prueba del modelo.  
Cada subconjunto contiene sus respectivas carpetas de im√°genes (`images/`) y etiquetas (`labels/`).

```
üìÇ TGC_RBNW/
‚îú‚îÄ‚îÄ üìÇ train/
‚îÇ   ‚îú‚îÄ‚îÄ üìÇ images/
‚îÇ   ‚îî‚îÄ‚îÄ üìÇ labels/
‚îú‚îÄ‚îÄ üìÇ val/
‚îÇ   ‚îú‚îÄ‚îÄ üìÇ images/
‚îÇ   ‚îî‚îÄ‚îÄ üìÇ labels/
‚îî‚îÄ‚îÄ üìÇ test/
    ‚îú‚îÄ‚îÄ üìÇ images/
    ‚îî‚îÄ‚îÄ üìÇ labels/
```

Para crear esta estructura, se desarroll√≥, con ayuda de la IA, un [**script en Python**](https://github.com/lauraheerrera/VC/blob/P4/Practica4//scripts/repartir_imagenes.py) que tom√≥ todas las im√°genes y etiquetas almacenadas inicialmente en la carpeta _todo_ y las dividi√≥ en tres subconjuntos de forma autom√°tica:
- **80%** del total del dataset se destin√≥ a **entrenamiento y validaci√≥n**.  
- **20%** restante se reserv√≥ para **pruebas (test)**.  
- Del **80% inicial**, se dividi√≥ de nuevo en:
  - **80%** para **entrenamiento (train)**
  - **20%** para **validaci√≥n (val)**
> [!IMPORTANT]
> Para ejecutar el script, sigue las instrucciones que se indican en la secci√≥n [Instrucciones para ejecutar el script](#script) ].

De esta forma, se garantiza una distribuci√≥n equilibrada y representativa del dataset, cumpliendo con las pr√°cticas recomendadas para el entrenamiento de modelos de detecci√≥n de objetos.

#### 4. De `json` a formato YOLO
Una vez creada la estructura de carpetas del dataset, es importante recordar  que las anotaciones generadas con **LabelMe** se guardan inicialmente en formato `.json`. Para que el modelo **YOLO** pueda utilizarlas, es necesario convertirlas al formato de etiquetas propio del framework.

Para ello, se desarroll√≥ un [**script en Python**]
(https://github.com/lauraheerrera/VC/blob/P4/Practica4/scripts/json_to_txt.py) que recorre todas las etiquetas en formato `.json` y las convierte en archivos `.txt` con la estructura est√°ndar de YOLO:
`<class_id> <x_center> <y_center> <width> <height>`

> [!IMPORTANT]
> Para ejecutar el script, sigue las instrucciones que se indican en la secci√≥n [Instrucciones para ejecutar el script](#script) ].

Cada l√≠nea del archivo `.txt` corresponde a un objeto detectado dentro de la imagen y contiene la siguiente informaci√≥n:
- **class_id** ‚Üí identificador num√©rico de la clase del objeto (por ejemplo, `0` para matr√≠culas).  
- **x_center** ‚Üí coordenada **x** del centro del contenedor delimitador.  
- **y_center** ‚Üí coordenada **y** del centro del contenedor delimitador.  
- **width** ‚Üí ancho del contenedor delimitador.  
- **height** ‚Üí alto del contenedor delimitador.  

Las coordenadas del centro (`x_center`, `y_center`) y las dimensiones (`width`, `height`) se encuentran **normalizadas**, es decir, divididas por el ancho y alto total de la imagen, para que sus valores est√©n comprendidos entre 0 y 1.

De esta forma, las etiquetas resultantes son totalmente compatibles con los modelos **YOLO**, permitiendo entrenar el detector de matr√≠culas de manera eficiente.

#### 5. Archivo de configuraci√≥n del dataset

Se cre√≥ un archivo [`data.yaml`](https://github.com/lauraheerrera/VC/blob/P4/Practica4/scripts/data.yaml), que define las rutas del conjunto de datos utilizadas durante el entrenamiento, validaci√≥n y prueba del modelo.  
Adem√°s, este archivo especifica el n√∫mero de clases y sus nombres, informaci√≥n necesaria para que **YOLO** interprete correctamente el dataset.

El contenido del archivo tiene la siguiente estructura:
```yaml
# TGCRBNW paths

train: C:/Users/laura/OneDrive/Desktop/VC/Practica4/TGC_RBNW/train/
val: C:/Users/laura/OneDrive/Desktop/VC/Practica4/TGC_RBNW/val/
test: C:/Users/laura/OneDrive/Desktop/VC/Practica4/TGC_RBNW/test/

# number of classes
nc: 1

# class names
names: [ 'license_plate' ]
```
---

<a name= "entrenamiento"></a>
### üìà Proceso para el entrenamiento YOLO
A continuaci√≥n, se entrenar√° el modelo YOLO:
#### 1. Activar el entorno para entrenamiento si no se ha hecho previamente
`conda activate VC_P4`

#### 2. Comprobar que la GPU est√° disponible
```bash
python -c "import torch; print('Torch version:', torch.__version__); print('CUDA available:', torch.cuda.is_available()); print('GPU:', torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'None')"
```

#### 3. Ejecutar entrenamiento YOLO
Desde la carpeta donde est√° `data.yaml` y las im√°genes:
```
cd "C:\Users\laura\OneDrive\Desktop\VC\Practica4"
```
1. Train 1 (T1) ‚Äì Entrenamiento r√°pido de referencia
```bash
yolo detect train model=yolo11n.pt data=data.yaml imgsz=512 batch=4 device=0 epochs=40 lr0=0.01
```
2. Train 2 (T2) ‚Äì Entrenamiento largo con im√°genes peque√±as
```bash
yolo detect train model=yolo11n.pt data=data.yaml imgsz=416 batch=4 device=0 epochs=100 lr0=0.001
```
3.  Train 3 (T3) ‚Äì Entrenamiento largo con resoluci√≥n media
```bash
yolo detect train model=yolo11n.pt data=data.yaml imgsz=512 batch=4 device=0 epochs=100 lr0=0.001
```
4. Train 4 (T4) ‚Äì Entrenamiento con im√°genes grandes y pocas √©pocas
```bash
yolo detect train model=yolo11n.pt data=data.yaml imgsz=640 batch=4 device=0 epochs=50 lr0=0.001
```
5. Train 5 (T5) ‚Äì Entrenamiento con batch grande
```
yolo detect train model=yolo11n.pt data=data.yaml imgsz=512 batch=8 device=0 epochs=60 lr0=0.001
```
6. Train 6 (T6) ‚Äì Repetici√≥n para comparar consistencia
```
yolo detect train model=yolo11n.pt data=data.yaml imgsz=416 batch=4 device=0 epochs=100 lr0=0.001
```
7. Train 7 (T7) ‚Äì Entrenamiento de alta resoluci√≥n, pocas √©pocas
```
yolo detect train model=yolo11n.pt data=data.yaml imgsz=768 batch=2 device=0 epochs=25 lr0=0.001
```
8. Train 8 (T8) ‚Äì Entrenamiento balanceado entre resoluci√≥n y duraci√≥n
```
yolo detect train model=yolo11n.pt data=data.yaml imgsz=640 batch=4 device=0 epochs=80 lr0=0.001
```
9. Train 9 (T9) ‚Äì Entrenamiento con learning rate alto
```
yolo detect train model=yolo11n.pt data=data.yaml imgsz=512 batch=4 device=0 epochs=100 lr0=0.01
```

Par√°metros:
- `model` ‚Üí modelo base/preentrenado (`yolo11n.pt`)
- `data` ‚Üí archivo YAML con rutas y clases (`data.yaml`)
- `imgsz` ‚Üí tama√±o de entrada de las im√°genes (512, 640, etc.)
- `batch` ‚Üí tama√±o de batch por iteraci√≥n (4, 8, etc.)
- `device=0` ‚Üí GPU utilizada (0 para la primera GPU, cpu si no hay GPU)
- `epochs` ‚Üí n√∫mero de √©pocas de entrenamiento (40, 100, etc.)
- `lr0` ‚Üí learning rate inicial para el entrenamiento (0.001, 0.01, etc.)
  
Se realizaron 9 entrenamientos para evaluar distintas combinaciones de tama√±o de imagen, n√∫mero de √©pocas, batch y learning rate:
- T1: Entrenamiento r√°pido de referencia, pocas √©pocas y tama√±o medio.
- T2: Largo con im√°genes peque√±as, para evaluar convergencia con menor detalle.
- T3: Largo con resoluci√≥n media, comparando precisi√≥n con T2.
- T4: Im√°genes grandes y pocas √©pocas, para capturar detalles sin mucho tiempo de entrenamiento.
- T5: Batch grande, probando estabilidad y suavidad de la convergencia.
- T6: Repetici√≥n de un entrenamiento largo, para validar consistencia de resultados.
- T7: Alta resoluci√≥n y pocas √©pocas, ideal para matr√≠culas peque√±as o lejanas.
- T8: Balance entre resoluci√≥n y duraci√≥n, buscando un modelo s√≥lido.
- T9: Learning rate alto, para observar efecto en velocidad de convergencia y estabilidad.

Este conjunto permite comparar c√≥mo cada par√°metro afecta la precisi√≥n y eficiencia del modelo de detecci√≥n de matr√≠culas.

<a name="resultados"></a>
### üìä Resultados del entrenamiento
Tras ejecutar los distintos entrenamientos, YOLO genera autom√°ticamente los resultados en la carpeta:
Dentro de esta carpeta, se crean subcarpetas por cada ejecuci√≥n, por ejemplo `train1`, `train2`, etc. Cada subcarpeta contiene los siguientes elementos:
- **`weights/`** ‚Üí Modelos entrenados:
  - **`best.pt`** ‚Üí Modelo que obtuvo la mejor precisi√≥n durante el entrenamiento.  
  - **`last.pt`** ‚Üí Modelo final despu√©s de completar todas las √©pocas, aunque no sea el m√°s preciso.
- **`results.png`** ‚Üí Gr√°fica que muestra la evoluci√≥n de las m√©tricas de entrenamiento: precisi√≥n, recall y loss.
- **`results.csv`** -> Registro √©poca por √©poca de las m√©tricas durante el entrenamiento y la validaci√≥n. Cada fila corresponde a una √©opca

Para determinar qu√© entrenamiento se considera el mejor, se ha tenido en cuenta las principales m√©tricas, que reflejan la calidad de la detecci√≥n:

| M√©trica                | Qu√© indica                                             | Consideraci√≥n para evaluaci√≥n                                                 |
| ---------------------- | ------------------------------------------------------ | ------------------------------------------------------------------ |
| `metrics/precision(B)` | Qu√© porcentaje de las predicciones fueron correctas    | M√°s alto = mejor                                                   |
| `metrics/recall(B)`    | Qu√© porcentaje de los objetos reales fueron detectados | M√°s alto = mejor                                                   |
| `metrics/mAP50(B)`     | Precisi√≥n promedio considerando IoU ‚â• 0.5              | M√°s alto = mejor; ideal >0.7 para muchos casos                     |
| `metrics/mAP50-95(B)`  | Precisi√≥n promedio considerando IoU entre 0.5 y 0.95   | M√°s robusta que mAP50, porque penaliza predicciones menos precisas |

Otras m√©tricas importantes son las de p√©rdidas, que reflejan qu√© tan bien aprende el modelo:
| M√©trica                           | Qu√© indica                                                  | Consideraci√≥n para evaluaci√≥n    |
| --------------------------------- | ----------------------------------------------------------- | ---------------- |
| `train/box_loss` y `val/box_loss` | Error de localizaci√≥n (qu√© tan bien encaja el bounding box) | M√°s bajo = mejor |
| `train/cls_loss` y `val/cls_loss` | Error de clasificaci√≥n (qu√© tan bien clasifica el objeto)   | M√°s bajo = mejor |
| `train/dfl_loss` y `val/dfl_loss` | Loss de distribuci√≥n focal (refina boxes)                   | M√°s bajo = mejor |

Para evaluar la calidad de un modelo, se ha priorizado las m√©tricas `*_best` de mAP y p√©rdidas de validaci√≥n, ya que reflejan el mejor rendimiento alcanzado durante el entrenamiento.

Sabiendo esto, se ha desarrolado otro [script de Python](https://github.com/lauraheerrera/VC/blob/P4/Practica4/scripts/guardar_metricas_yolo.py) que recorre autom√°ticamente todas las carpetas de entrenamiento (`train`,  `train2`, etc.), extrae las m√©tricas de cada ejecuci√≥n y genera un resumen de los mejores resultados para cada entrenamiento. Adem√°s, ordena autom√°ticamente los entrenamientos seg√∫n el siguiente criterio de prioridad, de manera que el primero en la lista corresponde al modelo mejor considerado:
1. `mAP50(B)` m√°s alto ‚Üí La m√©trica principal para determinar precisi√≥n de detecci√≥n.
2. `mAP50-95(B)` alto ‚Üí Eval√∫a robustez frente a predicciones menos perfectas.
3. Loss de validaci√≥n bajos (`val/box_loss`, `val/cls_loss`, `val/dfl_loss`) ‚Üí Indican que el modelo aprendi√≥ bien sin sobreajustarse.
4. Precision y recall equilibrados ‚Üí Evita falsos positivos o falsos negativos excesivos, asegurando un modelo confiable.


> [!IMPORTANT]
> Para ejecutar el script, sigue las instrucciones que se indican en la secci√≥n [Instrucciones para ejecutar el script](#script) ].

> De esta manera, al abrir el [Excel generado por el script](https://github.com/lauraheerrera/VC/blob/P4/Practica4/resumen_entrenamientos_mejores.xlsx), los entrenamientos aparecen ordenados seg√∫n estas prioridades, facilitando la identificaci√≥n del mejor modelo sin necesidad de revisar manualmente cada m√©trica.

La siguiente tabla muestra c√≥mo se presentan los entrenamientos en el _Excel_:
| Entrenamiento | train/box_loss_best | train/cls_loss_best | train/dfl_loss_best | val/box_loss_best | val/cls_loss_best | val/dfl_loss_best | metrics/precision(B)_best | metrics/recall(B)_best | metrics/mAP50(B)_best | metrics/mAP50-95(B)_best | val_loss_sum | pr_sum |
|---------------|------------------|-------------------|-------------------|-----------------|-----------------|-----------------|---------------------------|------------------------|----------------------|-------------------------|-------------|--------|
| train8        | 1.09635          | 1.2899            | 1.05174           | 0.87794         | 0.79134         | 0.96896         | 0.99986                   | 1.0000                 | 0.9950               | 0.74894                 | 2.63824     | 1.99986|
| train         | 0.96920          | 1.15768           | 0.98533           | 0.96195         | 0.83262         | 0.96145         | 0.99636                   | 1.0000                 | 0.9950               | 0.69216                 | 2.75602     | 1.99636|
| train3        | 0.96814          | 0.84988           | 0.95254           | 1.02529         | 0.75701         | 1.01806         | 0.99467                   | 1.0000                 | 0.9950               | 0.68005                 | 2.80036     | 1.99467|
| train9        | 0.96814          | 0.84988           | 0.95254           | 1.02529         | 0.75701         | 1.01806         | 0.99467                   | 1.0000                 | 0.9950               | 0.68005                 | 2.80036     | 1.99467|
| train2        | 0.72048          | 0.55734           | 0.89550           | 0.94331         | 0.60031         | 0.96383         | 1.00000                   | 0.94927                | 0.99204              | 0.70802                 | 2.50745     | 1.94927|
| train6        | 0.72048          | 0.55734           | 0.89550           | 0.94331         | 0.60031         | 0.96383         | 1.00000                   | 0.94927                | 0.99204              | 0.70802                 | 2.50745     | 1.94927|
| train4        | 0.83696          | 1.04597           | 0.90473           | 0.85140         | 0.76191         | 0.94132         | 0.95481                   | 1.0000                 | 0.99192              | 0.76919                 | 2.55463     | 1.95481|
| train5        | 0.74647          | 0.73564           | 0.91225           | 0.91703         | 0.70769         | 0.97270         | 0.96803                   | 0.9600                 | 0.99071              | 0.74413                 | 2.59742     | 1.92803|
| train7        | 0.94845          | 2.28946           | 0.95819           | 0.91889         | 1.43075         | 1.05861         | 0.95651                   | 0.9600                 | 0.98724              | 0.72895                 | 3.40825     | 1.91651|

Como se observa, **`train8` es el modelo recomendado para la detecci√≥n de matr√≠culas**, y los resultados obtenidos servir√°n como referencia para optimizar y ajustar futuras iteraciones del entrenamiento de YOLO.

<a name= "script"></a>
### Instrucciones para ejecutar un script  
1. Abre tu terminal
2. Sit√∫ate en la carpeta donde se encuentra el script. En mi caso: `cd "C:\Users\Laura\Desktop\VC\Practica 4"`
3. Ejecuta el script: `python <nombre_script>`

--- 
<a name="pr√°ctica4b"></a>
## Pr√°ctica 4b - Reconocimiento de car√°cteres
El objetivo de esta pr√°ctica, es ampliar el sistema desarrollado en la [Pr√°ctica 4](#practica4), dedicado a la detecci√≥n de veh√≠culos y matr√≠culas, a√±adiendo un reconocimieno √≥ptico de caracteres (OCR) para identificar las matr√≠culas visibles en los veh√≠culos detectados. 

Para ello, se har√° uso de dos modelos de detecci√≥n (YOLO):
- Modelo YOLO11vn: modelo preentrenado usado para detectar veh√≠culos y personas
- Modelo YOLO personalizado: entrenado previamente para detectar matr√≠culas dentro de los veh√≠culos. Este modelo es el elegido tras [comparar los resultados](#resultados).
Estos modelos permiten localizar y hacer tracking de cada persona y cada coche a lo largo del v√≠deo de entrada

<a name="OCR"></a>
### üî† Modelos OCR seleccionados
Para el reconocimiento de caracteres en las matr√≠culas se han seleccionado dos modelos OCR de distinto funcionamiento: Tesseract y EasyOCR.
La elecci√≥n de ambos responde al objetivo de comparar un enfoque basado en reglas y reconocimiento cl√°sico de caracteres (Tesseract), frente a un enfoque moderno basado en redes neuronales profundas (EasyOCR).

<a name="OCR-entorno"></a>
### üíª Preparaci√≥n del entorno OCR 
Para el uso de estos modelos, se necesita una instalaci√≥n previa:
- Tessaract:
    - Descargar los binarios desde [Universidad Manheim](https://github.com/UB-Mannheim/tesseract/wiki)
    - Ejecutar el archivo
    - Instalar el wrapper _pytesseract_ en el entorno creado:
      ```bash
      conda activate VC_P4
      pip install pytesseract
      ```
- EasyOCR
      ```
      pip install easyocr
      ```
Asimismo, es necesario la librer√≠a `pandas`, dise√±ada para trabajar con datos tabulares de manera eficiente:
    ```
    pip install pandas
    ```

<a name="proceso"></a>
### üîç Proceso de reconocimiento de clases y caracteres
El procesamiento del video se realiza frame a frame, siguiendo estos pasos:

**1. Carga de modelos YOLO**
- YOLO11n: preentrenado para detectar veh√≠culos y personas.
- YOLO personalizado: entrenado para detectar matr√≠culas dentro de los veh√≠culos.
  
**2. Lectura del video y configuraci√≥n de salida**
- Se abre el video de entrada y se obtienen sus propiedades (ancho, alto, FPS).
- Se crea un objeto para escribir el video resultante con las anotaciones de detecci√≥n y OCR.
  
**3. Detecci√≥n y tracking de veh√≠culos**
- Para cada frame, `YOLO11n` detecta veh√≠culos y personas.
- Se asigna un ID √∫nico a cada objeto para poder hacer tracking, es decir, seguirlo a lo largo de los frames.
- Se dibujan cajas y etiquetas sobre los objetos detectados.
  
**4. Recorte de ROI (Region of Interest)**

  Para reducir el √°rea de procesamiento y mejorar la precisi√≥n, se recorta la zona del coche detectado:
    ```python
    roi_car = frame[y1:y2, x1:x2]
    ```
  Dentro de esa ROI, el modelo de matr√≠culas busca la placa, que a su vez se recorta como ROI de la matr√≠cula:
  ```python
  matricula_roi = roi_car[my1:my2, mx1:mx2]
  ```
**5. Preprocesamiento de la matr√≠cula**

Antes de aplicar OCR, se mejora la imagen para facilitar la lectura de caracteres:
    - Conversi√≥n a escala de grises, pues OCR no necesita color, solo contraste.
    - Aumento de resoluci√≥n mediante interpolaci√≥n para que los caracteres peque√±os sean m√°s legibles.
    - Suavizado con filtros para reducir ruido.
    - Binarizaci√≥n (umbral adaptativo) para convertir la imagen a blanco y negro puro.
    - Inversi√≥n de colores para asegurar que el texto sea m√°s claro que el fondo.
    
**6. Reconocimiento OCR**

Se aplican dos m√©todos para comparar resultados:
- EasyOCR: modelo basado en redes neuronales, que devuelve texto y nivel de confianza.
- Tesseract: OCR cl√°sico con whitelist de caracteres alfanum√©ricos y configuraci√≥n adecuada para texto corto (tipo de matr√≠cula).
  
**7. Selecci√≥n del mejor resultado**: Se compara el texto detectado por ambos OCR y se elige el m√°s largo o completo.

**8. Dibujo y anotaci√≥n en el frame**: Se dibuja un recuadro sobre la matr√≠cula y se escribe el texto detectado.

**9. Almacenamiento de resultados**

- Cada frame y cada objeto detectado se almacena en un diccionario con coordenadas, ID, tipo de objeto, texto OCR y tiempos de procesamiento.
- Finalmente, todos los diccionarios se convierten en un DataFrame de pandas y se guardan en un CSV:
```pyhton
df = pd.DataFrame(resultados)
df.to_csv("resultados.csv", index=False, sep=";")
```
<a name="resultados-4b"></a>
### üìä Resultados
Los resultados obtenidos a partir del video de prueba muestran que ninguno de los modelos de OCR logr√≥ identificar las matr√≠culas de manera completa en la mayor√≠a de los veh√≠culos detectados. La principal causa parece estar relacionada con la calidad del video, que presentaba baja resoluci√≥n, movimiento y condiciones de iluminaci√≥n desfavorables, dificultando la lectura de los caracteres.

El resultado de ello se puede ver en:  
- [V√≠deo resultante generado](https://github.com/lauraheerrera/VC/blob/P4/Practica4/resultados/resultado.mp4)
- [CSV generado](https://github.com/lauraheerrera/VC/blob/P4/Practica4/csv_resultados/resultados.csv)

Por ello, para medir la precisi√≥n de cada modelo OCR (EasyOCR y Tesseract), se ha utilizado un conjunto de im√°genes de veh√≠culos en las que la matr√≠cula real corresponde al nombre del archivo.
En total, se procesaron todas las im√°genes del conjunto mediante el modelo de detecci√≥n YOLO, encargado de localizar el veh√≠culo y la regi√≥n de la matr√≠cula dentro de cada imagen.

Una vez detectada la matr√≠cula, se aplicaron ambos m√©todos OCR:
* **EasyOCR**, que utiliza redes neuronales profundas preentrenadas para el reconocimiento de texto en im√°genes.
* **Tesseract OCR**, un sistema basado en segmentaci√≥n y reconocimiento √≥ptico de caracteres.

De cada imagen se extrajo:
- La matr√≠cula reconocida por EasyOCR
- La matr√≠cula reconocida por Tesseract
- La matr√≠cula real (nombre del archivo)

Todos estos resultados se guardaron en un archivo CSV.
Posteriormente, se compararon las matr√≠culas reconocidas con la matr√≠cula real para calcular dos m√©tricas:
- Exactitud (accuracy): porcentaje de im√°genes en las que la matr√≠cula detectada coincide exactamente con la matr√≠cula real.
- Similitud media: grado de parecido entre el texto reconocido y el texto real, calculado mediante la raz√≥n de coincidencia de caracteres.

De este modo, las im√°genes sirvieron tanto para entrenar y evaluar la capacidad de detecci√≥n de las matr√≠culas, como para medir el rendimiento comparativo entre los dos sistemas OCR empleados.

Los resultados comparativos se representan en la siguiente gr√°fica, donde se muestra la exactitud y la similitud media obtenida por cada modelo OCR:

<p align="center">
  <img src="https://github.com/lauraheerrera/VC/blob/P4/Practica4/resultados/precision_ocr.png" alt="Gr√°fica de precisi√≥n OCR" width="50%">
</p>


<a name="tiempos"></a>
### ‚åõ Diferencia de los tiempos de inferencia
Dado el [CSV generado](https://github.com/lauraheerrera/VC/blob/P4/Practica4/resultados.csv), se puede hacer una peque√±a comparaci√≥n del tiempo de inferencia. Se conoce como tiempo de inferencia al tiempo que tarda un modelo en procesar un √∫nico frame y generar su resultado (en este caso, la detecci√≥n y reconocimiento de matr√≠culas).

A partir de los datos del CSV, se calcularon el **promedio** y la **desviaci√≥n est√°ndar** del tiempo de inferencia para los modelos EasyOCR y Tesseract:
| Modelo    | Promedio (s) | Desviaci√≥n est√°ndar (s) |
| --------- | ------------ | ----------------------- |
| EasyOCR   | 0,000595     | 0,00326                 |
| Tesseract | 0,00571      | 0,02713                 |

<div align="center">
<img width="443" height="180" alt="image" src="https://github.com/user-attachments/assets/87ad14d5-67e9-409e-9922-c93136653392" />
</div>

De estos resultados, se puede observar que:
- **Velocidad media**: EasyOCR es aproximadamente 10 veces m√°s r√°pido que Tesseract, con un tiempo medio pr√°cticamente despreciable frente a Tesseract.
- **Consistencia**: La desviaci√≥n est√°ndar de EasyOCR es mucho menor, lo que indica que sus tiempos de inferencia son muy estables entre distintos frames. Por el contrario, Tesseract presenta una mayor variabilidad, con algunos frames tardando significativamente m√°s que otros.

En conclusi√≥n, **EasyOCR** es mucho **m√°s eficiente en tiempo de inferencia**, mientras que **Tesseract**, aunque sigue siendo r√°pido, es **m√°s variable y tarda m√°s en promedio**. 

Esta informaci√≥n es importante a la hora de decidir qu√© modelo utilizar en aplicaciones donde la velocidad de procesamiento es cr√≠tica, como la detecci√≥n en tiempo real.

> Uso de la ia
- Generaci√≥n de scripts
- Explicaci√≥n de algunas funciones
- Ayuda con incompatibilidad de librer√≠as
- Estructura y redacci√≥n del Readme
