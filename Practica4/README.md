# Pr√°ctica 4 y 4b

**Autores:**  
- Laura Herrera Negr√≠n  
- Ayman Asbai Ghoudan

**Universidad:** Universidad de Las Palmas de Gran Canaria  
**Asignatura:** Visi√≥n por Computador  

---
## Contenidos
- [Librer√≠as utilizadas](#librerias)
- [Pr√°ctica 4 - Detecci√≥n de veh√≠culos y matr√≠culas](#pr√°ctica4) 
    - [Preparaci√≥n del dataset para YOLO](#dataset) 
    - [Entrenamiento YOLO](#entrenamiento)
- [Pr√°ctica 4b - ](#tarea2)
---

<a name= "librerias"></a>
## Librer√≠as utilizadas

--- 

<a name="pr√°ctica4"></a>
## Pr√°ctica 4 - Detecci√≥n de veh√≠culos y matr√≠culas
El objetivo de esta pr√°ctica es desarrollar un prototipo para detectar y seguir veh√≠culos y personas, as√≠ como la localizaci√≥n y reconocimiento de las matr√≠culas de dichos veh√≠culos a partir de un v√≠deo. Para ello, se han empleado modelos de detecci√≥n de objetos YOLO (You Only Look Once).

<a name="entorno"></a>
### Prearaci√≥n del entorno
Para evitar conflictos con librer√≠as y garantizar la compatibilidad con el OCR utilizado posteriormente, se cre√≥ un nuevo entorno de conda con Python 3.9.5:
```bash
conda create --name VC_P4 python=3.9.5
conda activate VC_P4
pip install ultralytics
pip install lap
```
El paquete Ultralytics permite acceder a las versiones m√°s recientes de YOLO (YOLOv11 y YOLOv12), facilitando tanto el uso de modelos preentrenados como el entrenamiento de modelos personalizados.
  
<a name= "dataset"></a>
### Preparaci√≥n del dataset para YOLO
Para la detecci√≥n de matr√≠culas, se decidi√≥ entrenar un modelo YOLO personalizado, ya que los modelos preentrenados no incluyen esta clase de objeto por defecto.

El proceso seguido fue el siguiente:
#### 1. Obtenci√≥n y preparaci√≥n del dataset
Se recopil√≥ un conjunto de im√°genes que contuvieran veh√≠culos con matr√≠culas visibles.  
Este dataset fue creado de forma colaborativa entre los miembros del equipo de la asignatura, garantizando la variedad de condiciones (√°ngulos, iluminaci√≥n, tipos de veh√≠culos, etc.).  
En total, se recopilaron 150 im√°genes, que se guardaron en la carpeta [_todo_](https://github.com/lauraheerrera/VC/tree/P4/Practica4/todo), para posteriormente etiquetarlas y, una vez etiquetadas organizarlas siguiendo la estructura de YOLO.

#### 2. Anotaci√≥n de im√°genes
Para anotar las matr√≠culas dentro de las im√°genes se utiliz√≥ la herramienta **LabelMe**, que permite dibujar regiones rectangulares alrededor del objeto de inter√©s (la matr√≠cula).  

Para el uso de esta herramienta, se cre√≥ otro entorno:
```bash
conda create --name=labelme python=3.9
conda activate labelme
pip install labelme
```
Una vez instalado, se tecle√≥ _labelme_ desde _AnacondaPrompt_ y se abri√≥ una interfaz intuitiva para anotar las zonas alrededor de las matr√≠culas presentes.

Cada imagen anotada genera un archivo `.json` con la informaci√≥n de las regiones seleccionadas.  

#### 3. Estructura de directorios
Las im√°genes recolectadas se organizaron siguiendo la estructura esperada por **YOLO** para el entrenamiento, validaci√≥n y prueba del modelo.  
Cada subconjunto contiene sus respectivas carpetas de im√°genes (`images/`) y etiquetas (`labels/`).

<pre>
üìÇ <a href="https://github.com/lauraheerrera/VC/tree/P4/Practica4/TGC_RBNW">TGC_RBNW/</a>
‚îú‚îÄ‚îÄ üìÇ <a href="https://github.com/lauraheerrera/VC/tree/P4/Practica4/TGC_RBNW/train">train/</a>
‚îÇ   ‚îú‚îÄ‚îÄ <a href="https://github.com/lauraheerrera/VC/tree/P4/Practica4/TGC_RBNW/train/images">images/</a>
‚îÇ   ‚îî‚îÄ‚îÄ <a href="https://github.com/lauraheerrera/VC/tree/P4/Practica4/TGC_RBNW/train/labels">labels/</a>
‚îú‚îÄ‚îÄ üìÇ <a href="https://github.com/lauraheerrera/VC/tree/P4/Practica4/TGC_RBNW/val">val/</a>
‚îÇ   ‚îú‚îÄ‚îÄ <a href="https://github.com/lauraheerrera/VC/tree/P4/Practica4/TGC_RBNW/val/images">images/</a>
‚îÇ   ‚îî‚îÄ‚îÄ <a href="https://github.com/lauraheerrera/VC/tree/P4/Practica4/TGC_RBNW/val/labels">labels/</a>
‚îî‚îÄ‚îÄ üìÇ <a href="https://github.com/lauraheerrera/VC/tree/P4/Practica4/TGC_RBNW/test">test/</a>
    ‚îú‚îÄ‚îÄ <a href="https://github.com/lauraheerrera/VC/tree/P4/Practica4/TGC_RBNW/test/images">images/</a>
    ‚îî‚îÄ‚îÄ <a href="https://github.com/lauraheerrera/VC/tree/P4/Practica4/TGC_RBNW/test/labels">labels/</a>
</pre>

Para crear esta estructura, se desarroll√≥, con ayuda de la IA, un [**script en Python**](https://github.com/lauraheerrera/VC/blob/P4/Practica4/repartir.py) que tom√≥ todas las im√°genes y etiquetas almacenadas inicialmente en la carpeta  
[_todo_](https://github.com/lauraheerrera/VC/tree/P4/Practica4/todo) y las dividi√≥ en tres subconjuntos de forma autom√°tica:
- **80%** del total del dataset se destin√≥ a **entrenamiento y validaci√≥n**.  
- **20%** restante se reserv√≥ para **pruebas (test)**.  
- Del **80% inicial**, se dividi√≥ de nuevo en:
  - **80%** para **entrenamiento (train)**
  - **20%** para **validaci√≥n (val)**

De esta forma, se garantiza una distribuci√≥n equilibrada y representativa del dataset, cumpliendo con las pr√°cticas recomendadas para el entrenamiento de modelos de detecci√≥n de objetos.

### 4. De `json` a formato YOLO
Una vez creada la estructura de carpetas del dataset, es importante recordar  que las anotaciones generadas con **LabelMe** se guardan inicialmente en formato `.json`. Para que el modelo **YOLO** pueda utilizarlas, es necesario convertirlas al formato de etiquetas propio del framework.

Para ello, se desarroll√≥ un [**script en Python**](https://github.com/lauraheerrera/VC/blob/P4/Practica4/script.py) que recorre todas las etiquetas en formato `.json` y las convierte en archivos `.txt` con la estructura est√°ndar de YOLO:
`<class_id> <x_center> <y_center> <width> <height>`

Cada l√≠nea del archivo `.txt` corresponde a un objeto detectado dentro de la imagen y contiene la siguiente informaci√≥n:
- **class_id** ‚Üí identificador num√©rico de la clase del objeto (por ejemplo, `0` para matr√≠culas).  
- **x_center** ‚Üí coordenada **x** del centro del contenedor delimitador.  
- **y_center** ‚Üí coordenada **y** del centro del contenedor delimitador.  
- **width** ‚Üí ancho del contenedor delimitador.  
- **height** ‚Üí alto del contenedor delimitador.  

Las coordenadas del centro (`x_center`, `y_center`) y las dimensiones (`width`, `height`) se encuentran **normalizadas**, es decir, divididas por el ancho y alto total de la imagen, para que sus valores est√©n comprendidos entre 0 y 1.

De esta forma, las etiquetas resultantes son totalmente compatibles con los modelos **YOLO**, permitiendo entrenar el detector de matr√≠culas de manera eficiente.

#### 5. Archivo de configuraci√≥n del dataset

Se cre√≥ un archivo [`data.yaml`](https://github.com/lauraheerrera/VC/blob/P4/Practica4/data.yaml), que define las rutas del conjunto de datos utilizadas durante el entrenamiento, validaci√≥n y prueba del modelo.  
Adem√°s, este archivo especifica el n√∫mero de clases y sus nombres, informaci√≥n necesaria para que **YOLO** interprete correctamente el dataset.

El contenido del archivo tiene la siguiente estructura:
```yaml
# TGCRBNW paths

train: C:/Users/laura/OneDrive/Desktop/VC/Practica4/TGC_RBNW/train/
val: C:/Users/laura/OneDrive/Desktop/VC/Practica4/TGC_RBNW/val/
test: C:/Users/laura/OneDrive/Desktop/VC/Practica4/TGC_RBNW/test/

# number of classes
nc: 1

# class names
names: [ 'license_plate' ]
```
---

<a name= "entrenamiento"></a>
### Proceso para el entrenamiento YOLO
